# EXP-2-PROMPT-ENGINEERING-

## Aim: 
Comparative Analysis of different types of Prompting patterns and explain with Various Test Scenarios

Experiment:
Test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios. 
Analyze the quality, accuracy, and depth of the generated responses.


## Algorithm:
<img width="968" height="291" alt="image" src="https://github.com/user-attachments/assets/c2c53d88-7b87-451f-a5d9-cb23611022fe" />
<img width="902" height="730" alt="image" src="https://github.com/user-attachments/assets/cd83179e-ab3f-48bc-a503-773c758b0f9f" />

## Output
# Scenario A: Broad / Unstructured Prompt
# Broad/uncleared prompt
<img width="1007" height="689" alt="image" src="https://github.com/user-attachments/assets/50dcf4c8-780a-4fa6-82e7-9cc8022fe6db" />

# clearer/basic prompt
<img width="911" height="674" alt="image" src="https://github.com/user-attachments/assets/721a7c46-6f8c-484c-8f86-d0ff01424b13" />

# Scenario B: Complex Reasoning (Math/Logic)
# Broad/uncleared prompt
<img width="884" height="396" alt="image" src="https://github.com/user-attachments/assets/2cab007f-83a0-415c-b5cf-5e711cc5eace" />

# clearer/basic prompt
<img width="967" height="309" alt="image" src="https://github.com/user-attachments/assets/328a5d1d-f213-4785-aab2-4cc326b65e3f" />

# Scenario C: Creative Writing
# Broad/uncleared prompt
<img width="909" height="722" alt="image" src="https://github.com/user-attachments/assets/139d52bb-05c4-47fe-9614-6b98461e4636" />

# clearer/basic prompt
<img width="872" height="776" alt="image" src="https://github.com/user-attachments/assets/f5fd0ed5-b2ca-498a-8850-3f0f036a8c10" />

# Scenario D: Knowledge-based Query
# Broad/uncleared prompt
<img width="901" height="786" alt="image" src="https://github.com/user-attachments/assets/1fffb03b-46c9-4e20-8e2c-a057fff306c6" />

# clearer/basic prompt
<img width="904" height="750" alt="image" src="https://github.com/user-attachments/assets/a8e78177-a592-40c1-b8d7-da190ea46ba0" />

## Conclusion

Broad / unstructured prompts → inconsistent, less reliable results.
Refined prompts → significantly improve accuracy, depth, and relevance.
Best strategies:
Instruction + CoT for technical/analytical tasks.
Role-Based + Few-Shot for creative and teaching scenarios.
Effective prompt engineering is crucial for maximizing LLM performance.

## Result
Broad prompts give general, less structured responses, while refined prompts produce clearer, more accurate, and deeper outputs.
